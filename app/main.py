import warnings

from dotenv import load_dotenv

import pandas as pd
import numpy as np

from sklearn.preprocessing import MinMaxScaler

import torch
from torch.utils.data import DataLoader, TensorDataset

from transformers import BertForSequenceClassification, BertTokenizer

from model.train import model_training
from preprocessing.prepare_data import scale_data, create_sequences
from utils.utils import metrics, plot_TSMixer_results
from hyperparams import configs

import pickle
import mlflow

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

import nltk
# nltk.download('all')

import os

warnings.filterwarnings('ignore')

load_dotenv()
scaler = MinMaxScaler()

# params
RANDOM_STATE = 42
train_size_ratio = 0.8
finetuned_model_path = os.getenv("finetuned_model_path") # path to the finetuned finbert model
configs = configs #optuna hyperparams

raw_news_path = os.getenv("raw_news_path")
raw_price_path = os.getenv("raw_price_path")
# Data used for finetuning the sentiment model (generated by ChatGPT-4o)
synthetic_data_finetune = os.getenv("synthetic_data_finetune")
df_predict = os.getenv("df_predict")

saved_model = os.getenv("saved_model")


EXPERIMENT_NAME = 'project1_TS_TSMIXER'
mlflow.set_tracking_uri("http://127.0.0.1:5000")
mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)


if __name__ == "__main__":

# ------------------------------------ Data prep!

    """

    # Too slove on local machine. For the full data please refere to colab.
    # In the following app I have divided them into seperate folders for easy usage
   
    # Converting the datetime
    news_saving_path = '../dataset/news_data_preprocessed'
    stock_saving_path = '../dataset/stock_price_data_preprocessed'

    try:
        df_news, symbol = df_date_convert(raw_news_path, news_saving_path)
        df_price, _ = df_date_convert(raw_price_path, stock_saving_path)
        
    except:
        os.makedirs(news_saving_path, exist_ok=True)
        os.makedirs(stock_saving_path, exist_ok=True)

        df_news, symbol = df_date_convert(raw_news_path, news_saving_path)
        df_price, _ = df_date_convert(raw_price_path, stock_saving_path)

    # Summarizing the news
    news_read_path = "../dataset/news_data_preprocessed"
    news_saving_path = "../dataset/news_data_summarized"

    try:
        df_news = from_csv_summarize(news_read_path, news_saving_path, df_news, symbol)
    except:
        os.makedirs(news_saving_path, exist_ok=True)
        df_news = from_csv_summarize(news_read_path, news_saving_path, df_news, symbol)

    # Sentiment
    news_read_path = "../dataset/news_data_summarized"
    news_saving_path = '../dataset/news_data_summarized_sentiment'

    model_sentiment = BertForSequenceClassification.from_pretrained(finetuned_model_path)
    tokenizer_sentiment = BertTokenizer.from_pretrained(finetuned_model_path)

    try:
        df_news = from_csv_sentiment(news_read_path, news_saving_path, df_news, model_sentiment, tokenizer_sentiment, device)

    except:
        os.makedirs(news_saving_path, exist_ok=True)
        df_news = from_csv_sentiment(news_read_path, news_saving_path, df_news, model_sentiment, tokenizer_sentiment, device)



    saving_path = "../dataset/price_news_integrate"
    stock_processed = '../dataset/stock_price_data_preprocessed'
    news_processed = '../dataset/news_data_summarized_sentiment'

    try:
      start_inte(df_news, df_price, symbol, saving_path)
    except:
      os.makedirs(saving_path, exist_ok=True)
      start_inte(df_news, df_price, symbol, saving_path)
    
    
    """

# ------------------------------------ Data predict + hypertune
      
    df_predict = pd.read_csv(df_predict)

    # move to data prep
    df_predict = scale_data(df_predict)
    df_predict[['close', 'volume']] = scaler.fit_transform(df_predict[['close', 'volume']])

    values = df_predict.drop('date', axis=1).values

# ------------------------------------ Prediction
    seq_length = 180 # Number of timesteps
    lr = 0.001
    epochs=50
    hidden_size = 64
    num_layers = 2
    output_size = 1

    model_path = os.path.join(saved_model, 'm_TSMixer.pkl')


    X, y = create_sequences(values, seq_length)
    train_size = int(X.shape[0] * train_size_ratio)
    X_train, y_train = X[:train_size], y[:train_size]
    X_test, y_test = X[train_size:], y[train_size:]

    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)
    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)

    with mlflow.start_run():

        # Log params
        mlflow.log_param("seq_length", seq_length)
        mlflow.log_param("lr", lr)
        mlflow.log_param("epochs", epochs)
        mlflow.log_param("hidden_size", hidden_size)
        mlflow.log_param("num_layers", num_layers)
        mlflow.log_param("output_size", output_size)
        mlflow.log_param("input_size", X_train.shape[2])

        # log data
        dataset = mlflow.data.from_pandas(
            X_train
        )
        mlflow.log_input(dataset, context="AAPL Time Series Data")

        try:
            print("Trying to load the model...")
            with open(saved_model+'m_TSMixer.pkl', 'rb') as f:
                model = pickle.load(f)
            print("Model loaded successfully.")

        except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e:
                print(f"Model loading failed: {e}")
                print(f"Training the model!")

                os.makedirs(saved_model, exist_ok=True) # creating a directory to save the model

                # Train the model
                model = model_training(values, X_train.shape[2], train_loader, lr, epochs, hidden_size, num_layers, output_size)

                mlflow.TSMixer.log_model(model, "TSMixer_model", input_example=X_test.sample(5))

                # Save the trained model
                with open(saved_model+'m_TSMixer.pkl', 'wb') as f:
                    pickle.dump(model, f)

                print("Model trained and saved successfully.")

        model.eval()

        predictions = []
        with torch.no_grad():
            for batch_X, _ in test_loader:
                pred = model(batch_X)
                predictions.append(pred)

        # Convert predictions to a numpy array and invert normalization
        # Create a placeholder for inverse scaling
        predictions = torch.cat(predictions).numpy()

        # Initialize an array of zeros with the same number of features as the original data
        predicted_prices_full = np.zeros((predictions.shape[0], df_predict[['close', 'volume']].shape[1]))

        # Place the predictions in the 'Close' column (3rd index)
        predicted_prices_full[:, 0] = predictions[:, 0]

        # Inverse transform
        predicted_prices_full = scaler.inverse_transform(predicted_prices_full)

        # Extract the 'Close' prices from the inverse-transformed data
        predicted_prices = predicted_prices_full[:, 0]

        # Convert predictions to a DataFrame and save to a CSV file
        predicted_prices_df = pd.DataFrame(predicted_prices, columns=['predicted_close'])


    # ---------------------------------------------- Evaluation

        y_test_np = y_test.numpy().reshape(-1, 1)

        # Create an array with the same shape as the original features, filled with zeros
        y_test_full = np.zeros((y_test_np.shape[0], df_predict[['close', 'volume']].shape[1]))

        # Place y_test_np in the Close column (assuming it's the 4th column as before)
        y_test_full[:, 0] = y_test_np[:, 0]

        # Apply inverse transform only on the relevant column
        actual_prices_full = scaler.inverse_transform(y_test_full)

        # Extract the actual Close prices
        actual_prices = actual_prices_full[:, 0]

        # Evaluate the model
        results_eval, mae, mse, r2, mape = metrics(test=actual_prices, prediction=predicted_prices)
        print(results_eval)

        # log metrics

        mlflow.log_metric("MAE", mae)
        mlflow.log_metric("MSE", mse)
        mlflow.log_metric("R2", r2)
        mlflow.log_metric("MAPE", mape)

        # Matching predictions with dates
        predicted_dates = df_predict['date'].iloc[train_size + seq_length:].reset_index(drop=True)

        # Combine dates, actual prices, and predicted prices into a DataFrame
        predicted_prices_df = pd.DataFrame({
            'date': predicted_dates,
            'Actual_Close': actual_prices,
            'Predicted_Close': predicted_prices
        })

        # Visualization
        fig = plot_TSMixer_results(predicted_prices_df=predicted_prices_df, date='date', actual='Actual_Close', predicted='Predicted_Close')

        print(fig)
        mlflow.log_figure(fig, "Actual vs Predicted Close Prices")

